---
title: "Posterior Sampling with MCMC"
author: "Andy Shen, Devin Francom"
date: "LANL: CCS-6"
output: pdf_document
---


# Tasks

Say you have $\mathbf{y}_1,\dots,\mathbf{y}_n \sim N(\boldsymbol{\mu},\mathbf{\Sigma})$ where each $\mathbf{y}_i$ is a vector of length $p$. Use $n=100$, $p=3$, $\boldsymbol{\mu} = (1, 2,3)$ and

$$
\mathbf{\Sigma} =
\begin{pmatrix}
1.0 & 1.4 & 2.1 \\ 
  1.4 & 4.0 & 4.2 \\ 
  2.1 & 4.2 & 9.0 \\ 
\end{pmatrix}
$$

to generate some data

**Task 1:** Use $\mathbf{y}_1,\dots,\mathbf{y}_n \sim N(\boldsymbol{\mu},diag(\sigma^2_1,\dots,\sigma^2_p))$ as your likelihood, with $\boldsymbol{\mu} \sim N(\mathbf{m},\mathbf{S})$ as your prior for $\boldsymbol{\mu}$ and $\sigma^2_i \sim InvGamma(a,b)$ as your prior for $\sigma^2_i$.  Use Gibbs sampling to sample the resulting posterior.

**Task 2:** Now use $\mathbf{y}_1,\dots,\mathbf{y}_n \sim N(\boldsymbol{\mu},\mathbf{\Sigma})$ as your likelihood, with $\boldsymbol{\mu} \sim N(\mathbf{m},\mathbf{S})$ as your prior for $\boldsymbol{\mu}$ and the diagonal elements of $\mathbf{\Sigma}$ as $\sigma^2_i \sim InvGamma(a,b)$ and each correlation parameter as $\rho_{ij} \sim Beta(a,b)$.  Use Gibbs sampling and Metropolis-Hastings to sample the resulting posterior.

\pagebreak

# Data Generation

```{r}
set.seed(12)
library(mvtnorm)
library(invgamma)
library(dplyr)
library(truncnorm)
mu <- c(1, 2, 3); mu
sig <- cbind(c(1, 1.4, 2.1), c(1.4, 4.0, 4.2), c(2.1, 4.2, 9.0)); sig
data <- rmvnorm(1000, mu, sig)
n <- nrow(data)
str(data)
```

```{r, include=TRUE}
source("mcmc_gibbs_script.R")
```

\pagebreak

# Task 1

Our likelihood follows a multivariate normal distribution with mean $\boldsymbol{\mu}$ and variance $diag(\sigma^2_1,\dots,\sigma^2_p)$.

We multiply the likelihood by both priors to get our posterior distribution, $P(\boldsymbol{\mu}, \mathbf{\tilde\Sigma}  \mid \alpha, \beta,  \boldsymbol{m}, \boldsymbol{S})$, where $\mathbf{\tilde\Sigma} = diag(1, 4, 9)$.

We get the following result:

$$
P(\boldsymbol{\mu}, \mathbf{\tilde\Sigma}  \mid \alpha, \beta,  \boldsymbol{m}, \boldsymbol{S}) \propto
exp((\boldsymbol{\mu}- \mathbf{m})' \boldsymbol S^{-1} (\boldsymbol{\mu}- \mathbf{m}))
\prod_{i=1}^{P=3}\lbrace (\sigma_i^{2})^{-\alpha - 1} exp(-\frac{\beta}{\sigma_i^2}) \rbrace
\prod_{i=1}^{100} \lbrace \prod_{j=1}^{3}[{\sigma_j^2}^{-1/2}] exp\lbrace -\frac{1}{2}(\mathbf y_i-\boldsymbol\mu)' \mathbf{\tilde\Sigma}^{-1}(\mathbf y_i-\boldsymbol\mu)\rbrace \rbrace 
$$


Our $\sigma^2_i$ values follow an inverse gamma distribution with parameters $\alpha + 50$ and $\beta + \frac{1}{2} \sum_{j=1}^{100} (y_{ji} - \mu_i)^2$

$$
\sigma^2_i \sim IG(\alpha + 50, \beta + \frac{1}{2} \sum_{j=1}^{100} (y_{ji} - \mu_i)^2)
$$

Our $\mu_i$ values follow a univariate normal distribution:

$$
\mu_i \sim N(\frac{\sigma_i^2}{\sigma_i^2 + 100s_i^2}m_i + \frac{s_i^2\sum_{j=1}^{100}y_j}{\sigma_i^2 + 100s_i^2}, 
\frac{\sigma_i^2 s_i^2}{\sigma_i^2 + 100s_i^2})
$$


## Gibbs Sampling

Please see the `mcmc_gibbs_script.R` file for the code used to generate these results.

```{r}
res_easy <- gibbs_easy()
matplot(res_easy[[1]], type = "l", main = "Plot of mu values vs. iterations")
matplot((res_easy[[2]]), type = "l", main = "Plot of sigma^2 values vs. iterations")
```

\pagebreak

# Task 2: The Hard Task

Our likelihood follows a multivariate normal distribution with mean $\boldsymbol{\mu}$ and variance $diag(\sigma^2_1,\dots,\sigma^2_p)$.

We multiply the likelihood by both priors to get our posterior distribution, $P(\boldsymbol{\mu}, \mathbf{\Sigma}  \mid \alpha, \beta,  \boldsymbol{m}, \boldsymbol{S})$, where 

$$
\boldsymbol{\mu} = \begin{pmatrix}1\\2\\3\end{pmatrix}
$$
and

$$
\mathbf{\Sigma} = \begin{pmatrix}
1.0 & 1.4 & 2.1 \\ 
1.4 & 4.0 & 4.2 \\ 
  2.1 & 4.2 & 9.0 \\ 
\end{pmatrix}
$$

Our values of $\boldsymbol S$ and $\boldsymbol m$ used in the prior of $\boldsymbol \mu$.

```{r}
S <- diag(3)
Sinv <- solve(S)
m <- c(0,0,0)
```

\  

It follows that

$$
\boldsymbol\mu \mid \cdot ~~ \sim ~~ \mathcal{N}\big[~(\mathbf S^{-1} + n\boldsymbol\Sigma^{-1})^{-1} (\mathbf S^{-1}\boldsymbol m + n\boldsymbol\Sigma^{-1}\boldsymbol{\bar y}), ~~(\mathbf S^{-1} + n\boldsymbol\Sigma^{-1})^{-1}~\big]
$$

which is the full conditional distribution for $\boldsymbol \mu$.

\  

Recall that for our correlation coefficient $\rho_i$, we have that

$$
\rho_{ij} = \rho_{ji} = \frac{cov(i,j)}{\sigma_i \sigma_j}
$$

which we simplify to $\rho_i$ and work with the upper and lower triangular portion of the matrices.\newline 

## Metropolis-Hastings and Gibbs

Please see the `mcmc_gibbs_script.R` file for the code used to generate these results.

```{r, warning=FALSE}
its <- 1000
a <- met_gibbs(its = its)
```

\pagebreak

# Comparison of Results

## Mean Vector

Our acceptance rates of $\sigma^2_i$ and $\rho^2_i$, respectively.

```{r}
a[[4]] / its #sigma
a[[5]] / its #rho
```

\  

A plot of our sampled values of $\boldsymbol\mu$ is shown below, along with the column means:

```{r}
matplot(a[[1]], type = "l", main = "Plot of mu values vs. iterations")
```

\  

Column means of our sampled values:

```{r}
colMeans(a[[1]])
```

\  

The true mean from the generated data:

```{r}
colMeans(data)
```

\pagebreak

## Covariance Matrix

For $\sigma^2_i$, the plot of the sampled data and their column means is shown below.

```{r}
matplot((a[[2]]), type = "l", main = "Plot of sigma^2 values vs. iterations")
```

\  

Column means of sampled values:

```{r}
colMeans(a[[2]]) #average sigma^2 values
```

\  

Final covariance matrix $\boldsymbol{\hat\Sigma}$

```{r}
.cov <- a[[6]]; .cov #sampled
```

\  

Compare this to the true values:

```{r}
cov_data <- cov(data); cov_data #true values
```

\pagebreak

## Correlation Coefficients

Finally, for $\rho_i^2$, our results are as follows:

```{r}
matplot(a[[3]], type = "l", main = "Plot of rho values vs. iterations")
```

\  

Column means of sampled values:

```{r}
colMeans(a[[3]]) #average rho values
```

\  

We convert our covariance matrices to correlation matrices of our true and sampled data.\newline

Final values of rho:

```{r}
cor_sampled <- cov2cor(.cov); cor_sampled #sampled values
cor_sampled[upper.tri(cor_sampled)]
```

\  

Compare this to the true correlation matrix:

```{r}
cor_true <- cov2cor(cov_data); cor_true #true values
cor_true[upper.tri(cor_true)]
```

\  

As seen here, our sampled values of $\boldsymbol\mu$, $\sigma^2_i$ and $\rho_i$ closely match the true value from the generated data.


